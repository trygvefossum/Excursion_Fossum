\documentclass[aoas]{imsart}

\input{commands.tex}
\usepackage{amsmath}
\usepackage{pstricks,pst-grad}
\usepackage{graphicx}
\usepackage{floatrow}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\floatsetup[table]{capposition=top}
\usepackage{subfigure}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}                     % horizontal lines in tables
\usepackage{comment}

% == Enable text degreehttps://www.overleaf.com/project/5a3268379ecbdc657d8767e8
\usepackage{textcomp}

\usepackage{amsthm,amsmath,natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

% == Trygve Test
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}

% put your definitions there:
\startlocaldefs
\newcommand{\edcomment}[1]{{\color{green}{\{Editor: #1\}}}}
\newcommand{\frevcomment}[1]{{\color{blue}{\{Rev 1: #1\}}}}
\newcommand{\srevcomment}[1]{{\color{red}{\{Rev 2: #1\}}}}
\newcommand{\trevcomment}[1]{{\color{violet}{\{Rev 3: #1\}}}}
\endlocaldefs

% New Marcos.
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cleveref}
\input{macros}

\begin{document}

\begin{frontmatter}

% "New Title of the paper"
\title{Autonomous Oceanographic Data Collection towards Estimating Excursion Sets of Vector-valued Random Fields} 
\runtitle{Autonomous Oceanographic Data Collection}

% "Former Title of the paper" (DG 08.06.2020)
%\title{Autonomous Oceanographic Sampling Designs using Excursion Sets for Multivariate Gaussian random fields} \runtitle{Excursion Probabilities for Informative Sampling}

\begin{aug}
\author{\fnms{Trygve Olav} \snm{Fossum}\thanksref{t1,t2}, \corref{} \ead[label=e1]{trygve.o.fossum@ntnu.no}}
\author{\fnms{Cédric} \snm{Travelletti}\thanksref{t3}, \corref{} \ead[label=e2]{cedric.travelletti@stat.unibe.ch}}
\author{\fnms{Jo} \snm{Eidsvik}\thanksref{t4}, \ead[label=e3]{jo.eidsvik@ntnu.no}}
\author{\fnms{David} \snm{Ginsbourger}\thanksref{t3}, \ead[label=e4]{david.ginsbourger@stat.unibe.ch}}
\and
\author{\fnms{Kanna} \snm{Rajan}\thanksref{t5}. \ead[label=e5]{kanna.rajan@fe.up.pt}}

\affiliation[t1]{Department of Marine Technology, The Norwegian University of Science and Technology (NTNU), Trondheim, Norway.} 
\affiliation[t2]{Centre for Autonomous Marine Operations and Systems, NTNU.}
\affiliation[t3]{Institute of Mathematical Statistics and Actuarial Science, University of Bern, Switzerland.}
\affiliation[t4]{Department of Mathematical Sciences, NTNU.}
\affiliation[t5]{Underwater Systems and Technology Laboratory, Faculty of Engineering, University of Porto, Portugal.}

\address{\\Trygve Olav Fossum \\Department of Marine Technology\\ Otto Nielsens veg. 10, 7491 Trondheim\\ Norway\\
\printead{e1}}
\address{Cédric Travelletti\\ Institute of Mathematical Statistics and Actuarial Science \\ University of Bern \\
Switzerland.
\printead{e2}}
\address{Jo Eidsvik\\Department of Mathematical Sciences\\ Hogskoleringen 1, 7491 Trondheim\\ Norway\\ \printead{e3}}
\address{David Ginsbourger\\ Institute of Mathematical Statistics and Actuarial Science \\ University of Bern \\
Switzerland.
\printead{e4}}
\address{Kanna Rajan\\Underwater Systems and Technology Laboratory,
  Faculty of Engineering,\\ Rua Dr. Roberto Frias\\ University of Porto, Portugal\\
\printead{e5}}

\runauthor{TO. Fossum et al.}
\end{aug}

\begin{abstract}

  Improving and optimizing oceanographic sampling is a crucial task
  for marine science and maritime management. Faced with limited
  resources to understand processes in the water-column, the
  combination of statistics and autonomous robotics provides new
  opportunities for experimental design. In this work we develop
  methods for efficient spatial sampling applied to the mapping of
  coastal ocean processes by providing informative descriptions of
  spatial characteristics of ocean phenomena. Specifically, we define
  a design
  criterion based on improved characterization of the uncertainty in
  the excursions of vector-valued Gaussian random fields, and derive
  tractable expressions for the expected Bernoulli variance reduction
  in such a framework. We demonstrate how this criterion can be used
  to prioritize sampling efforts at locations that are ambiguous,
  making exploration more effective. We use simulations to study the
  properties of methods and to compare them with state-of-the-art
  approaches, followed by results from field deployments with an
  autonomous underwater vehicle (AUV) as part of a study mapping the
  boundary of a river plume. The results demonstrate the potential of
  combining statistical methods and robotic platforms to effectively
  inform and execute data-driven environmental sampling.
  
%Motivated by the challenges related to efficient allocation of sampling resources in environmental sensing, the combination of Excursion Probabilities and Gaussian process modeling is explored for autonomous robotic sampling of ocean features; enabling information driven measures  sampling efforts to high-interest regions. These regions are usually characterized by gradients of measurable environmental variables, e.g., temperature or salinity gradients, on which EPs subsequently can be used...Correlation among samples and multivariate requirements are typical in environmental studies.

\end{abstract}

\begin{keyword}
\kwd{Ocean Sampling}
\kwd{Excursion Sets}
\kwd{Gaussian Processes}
\kwd{Experimental Design}
\kwd{Autonomous robots}
\kwd{Adaptive Information Gathering}
\end{keyword}

\end{frontmatter}
\section{Introduction}

%NB: The subsections are used as a temporary instrument to highlight the proposed structure and should probably be abolished upon convergence to a stable version.

Motivated by the challenges related to efficient data collection strategies for the vast oceans, the combination of Gaussian process modeling is explored for autonomous robotic sampling of ocean features; enabling information driven measures  sampling efforts to high-interest regions.
This combination, of statistical tools and robotic platforms, constitute a methodological basis for addressing the problem of efficient sampling of the ocean, which is at the core of this work.

\subsection{Oceanic data collection and spatial design of experiments}



Monitoring the world's oceans has gained increased importance in light of the changing climate and increasing anthropogenic impact. 

Central to understanding the changes taking place in the upper
water-column is knowledge of the bio-geophysical interaction driven by
an agglomeration of physical forcings (e.g. wind, topography,
bathymetry, tidal influences, etc.) and incipient micro-biology driven
by planktonic and coastal anthropogenic input, such as pollution and
agricultural runoff transported into the ocean by rivers and streams.
These often result in a range of ecosystem-related phenomena such as
blooms and plumes, with direct and indirect effects on society. \kc{we
  should look for a citation here. Check with John Ryan}
 One of the bottlenecks in the study of such phenomena lies however in the lack of observational data with sufficient resolution. Most of this \emph{undersampling} can be attributed to the large spatio-temporal variations in which ocean processes transpire, prompting the need for effective means of data collection. 
%
 \textcolor{blue}{By \emph{sampling}, we refer here primarily to the design of observational strategies in the spatial domain %where the use autonomous robotic platforms can be combined with statistical methods 
 with the aim to pursue measurements with high scientific relevance}.
Based on collected data and sometimes also of prior knowledge, models from spatial statistics can be used to make predictions of the state variables under consideration over regions of interest. In case of multiple continuous state variables being jointly studied relying on spatially scattered data, vector-valued random field models and associated prediction approaches such as co-Kriging \citep[See, e.g.,][]{Wackernagel2003} can be considered nowadays as part of the standard statistical toolkit in oceanographic research and beyond. %applications and beyond.  
 %By \emph{sampling}, we refer to the design of observational strategies in the spatial domain, where the use autonomous robotic platforms can be combined with statistical methods to pursue measurements with high scientific relevance. This combination, of statistical tools and robotic platforms, constitute a methodological basis for addressing the problem of efficient sampling of the ocean, which is at the core of this work.
  


Data collection to understand natural and artificial phenomena at sea has typically been based on static buoys, floats, or ship-based methods,
with significant logistical limitations that directly impact coverage
and sampling resolution. Modern methods using satellite remote-sensing
provide large-scale coverage but have limited resolution, are limited
to sensing the surface, and are impacted by cloud cover. Numerical
ocean models similarly find it challenging to provide detail at fine
scale \citep{Lermusiaux:2006}, and also come with computation costs that can be practically limiting. 
%limiting computing resources, no time for hard-core ocean model Navier-Stokes, hence GPs and approximations, etc.
%
\begin{figure}[!h] 
  \centering 
  \subfigure[Illustration of an ocean sensing network and the sense-plan-act control methodology.]{\includegraphics[width =
    0.49\textwidth]{Figures/envir.pdf}\label{fig:envir1}}
  \hfill
  \subfigure[Frontal patterns off of the Nidelva river, Trondheim, Norway.]{\includegraphics[width =
    0.49\textwidth]{Figures/river_proccess.pdf}\label{fig:nidelven}}
  \caption{\ref{fig:envir1} Traditional ocean observation based on 
    ship-based sampling has been augmented by autonomous
    robotic vehicles. % and their interactions.  
    AUV platforms are an integral part of this network being able to
    reason and make decisions for efficient onboard adaptive sampling.
    % using the sense-plan-act control approach to autonomous control. 
    \ref{fig:nidelven} The interaction of river and ocean creates
    processes that are challenging to map, where the combination of
    statistics and robotics can play a vital role in enabling more
    effective oceanographic observation.\kc{The inset ``AUV robot''
      looks very tacky and ad-hoc; might be best to remove it. Change
      ``ships'' to
      ``Ship'', remove the aerial drone, since we didn't use it and
      don't plan to in this work.}}
  \label{fig:envir} \end{figure}
%
\begin{comment} % Somehow redundant with the parag following below
Mobile robotic platforms \citep{Bellingham07} have contributed significantly to environmental monitoring and sampling. In particular, AUVs have advanced the state of in-situ sampling, and made robotics an integral part of ocean observation, filling parts of the undersampling gap \cite{rudnick03,rudnick18} (See Fig. \ref{fig:envir1}).
\end{comment}
%
The advent of robust mobile robotic platforms \citep{Bellingham07} has
resulted in significant contributions to environmental monitoring and
sampling in the ocean. In particular, autonomous underwater vehicles (AUVs) have advanced the state of data collection and consequently have made robotics an integral part of ocean observation; %our 
previous work by some of the authors of the present paper have contributed to this effort \citep{das11b,Das2015,fossuminformation,fossum18b}. 


As full numerical ocean models based on complex differential equations cannot be run onboard a marine robot with limited computational capacity, statistical models relying on collected data and random field assumptions appear quite relevant as means to guide AUV data collection trajectories. Our main research angle in the present work is to extend sequential design strategies from the world of spatial statistics and computer experiments to frameworks featuring both vector-valued observational data and experimental designs that may be constrained by the nature of feasible trajectories.      
\\

Surveys with AUVs are usually limited to observations along fixed
transects that are pre-scripted in mission plans created manually by a
human operator. Missions can be specified operating on a scale of
hundreds of meters to tens of kilometers depending on the scientific
context. Faced with limited coverage capacity, a more effective
approach is to instead use onboard algorithms to continuously
evaluate, update, and refine future sampling locations (sense-plan-act
cycle in Fig. \ref{fig:envir1}), making the information gathering
\emph{adaptive} \citep{das11b,Das2015,fossuminformation,fossum18b}.
% This usually occurs on a spatial grid, called a waypoint graph. 
In doing so, the space of sampling opportunities is still limited by
the so-called waypoint graph, a discretization of the search domain, but the AUV has flexibility to modify its path at each waypoint based on in-situ sensing and measurements onboard \citep{py10,Rajan12,Rajan12b}.\\

The work presented here is primarily inspired by a case study 
%application where an AUV is used to measure both temperature and salinity in 
pertaining to the spatial characterization of a frontal system generated by
river plumes. Fig. \ref{fig:nidelven} shows the survey area in
Trondheim, Norway, where cold freshwater enters from the river,
creating a strong gradient in both temperature and salinity. Because
of the local topography and the Coriolis force the cold fresh water
tends to flow to the east. Depending on the river discharge, tidal
effects, wind, and temperature differences, this boundary often gets
distorted. Initial knowledge about the location and evolution of these
features are highly uncertain, making deterministic planning
challenging. Here the goal is to use AUV measurements towards a better description of the interface between freshwater versus oceanic water, with the help of a vector-valued random field model for both temperature and salinity. As we will present in the next sections, some of the original contributions inspired by this specific problem are actually applicable to further settings and application areas. \\

The questions tackled here hence pertain to the broader area of spatial data collection and experimental design, yet with the particularity of estimating some regions of the domain, mostly excursion sets, being implicitly defined by a scarcely observed vector-valued random field. 
%because of limited resources to sample the large oceanographic domains, one must plan for active learning during the operation, where a relevant criterion is used to extract the most valuable designs. The acquired in-situ information must then be assimilated into the statistical model onboard the platform, so that it can be used to re-compute the sampling criterion and in this way inform decisions on where to sample next. \\
Moreover, given constraints on AUV movements and the fact that surveys rely by design in such a framework on successive measurements spread along a trajectory, addressing corresponding design problems calls for particular sequential strategies, possibly also accounting for the finiteness of resources by anticipating multiple steps ahead and thereby improving over myopic strategies. \\

Here we aim to leverage and extent recent progress in uncertainty quantification and reduction for excursion sets of Gaussian random fields in order to address vector-valued observational cases under constrained design such as in the motivating application of designing AUV trajectories to better distinguish between freshwater from ocean waters, relying on temperature and salinity measurements. %approach will take substantial advantage of existing work in the field of excursion set estimation mainly dedicated to computer experiments in the scalar-valued case, and where space exploration can be performed without constraints regarding the distance between successive design points. 
Original contributions compared to recent related work in the framework of scalar-valued (computer) experiments include: 
\begin{itemize}
    \item Extension of two important Sequential Uncertainty Reduction criteria of \citep{Bect.etal2012,chevalier2014fast,bect2019} to homotopic or heterotopic vector-valued settings,
    \item Investigation of myopic and multiple-step ahead batch-sequential strategies for optimizing trajectories with respect to such criteria,
    \item Replicable experiments with synthetic test cases and original data sets with accompagnying codes. 
\end{itemize}
Let us briefly review random field modelling and recent advances in targetd sequential design of experiments based on Gaussian Processes before detailing further existing literature tackling the considered problems, our proposed approach, as well as outlining the rest of the paper.  


\subsection{Random field modelling and targeted sequential design of experiments}
 
 While random field modelling has been one of the main topics throughout the history of spatial statistics \citep{Krige1951a, Matheron1963, Stein1999, Adler.Taylor2007}, there has recently been a renewed interest for random field models in the context of static sequential experimental design, be it in the context of spatial data collection \citep{Mueller2007} or in computer experiments and machine learning \citep[See for instance][and references therein]{Santner.etal2003, Romero.etal2013, Beck2016}. As detailed in \cite{Ginsbourger2018}, Gaussian random field models have been used in particular as a basis to sequential design of comupter experiments dedicated to various goals such as global optimization but also set estimation.  \cite{Picheny.etal2010} revisited the classical Integrated Mean Square Error criterion with an adaptation dedicated to contour line estimation, \cite{Bect.etal2012} focused on strategies to reduce uncertainties on volumes of excursion, i.e. on the measure of the domain of input parameters which associated (scalar) response level exceeds a prescribed threshold, followed up by \cite{chevalier2014fast} presenting computational contributions towards an efficient batch-sequential implementation of strategies introduced in the former. Meanwhile, rather than focusing on excursion volumes, some approaches were investigated in \cite{French.Sain2013,Chevalier.etal2013b,Bolin.Lindgren2015,Azzimonti.etal2016} that ambition to estimate sets themselves. Recently, sequential design of experiments for the conservative estimation of excursion sets based on Gaussian random field models was presented in \citep{Azzimonti.etal}. 
 %
Surprisingly less attention has been dedicated to sequential strategies in the case of vector-valued observations to our knowledge. It has been acknowledge for several decades that co-Kriging models could be efficiently updated with vector-valued observations being assimilated sequentially 
\cite{Vargas-Guzman1999}, sequential strategies targeted to estimate prescribed features of vector-valued random fields are not in still in their infancy. We can refer for instance to \citep{LeGratiet.etal2015} for co-Kriging-based sequential design using fast cross-validation and dedicated to multi-fidelity computer codes. Related ideas are to be found in \cite{Poloczek2017} in the context of Multi-Information Source Optimization. As for extensions of Stepwise Uncertainty Reduction strategies, to our knowledge the only attempt so far to generalize excursion-related Stepwise Uncertainty Reduction criteria (such as reviewed above) in vector-valued settings pertains to the design of multi-fidelity simulations for fire safety, in the PhD thesis \citep[][p.82]{stroh}, where the outputs are mainly assumed independent yet the feasibility of treating the dependent case by appealing to higher-dimensional Gaussian CDF evaluations is mentioned. 


% From earlier version; to be relocated?
%Hence statistical proxy models of the environment must be used. For our purpose, we rely on Gaussian process (GP) representations of the ocean variables of interest because they are computationally convenient and yet provide enough flexibility to realistically model the spatial variability and dependence, and the correlation between multiple processes. 
\subsection{Contributions, selected previous work on AUV sampling, and outline} 
%work and proposed approach in a nutshell}
%\begin{itemize}
%    \item 
%\end{itemize}
%\textcolor{blue}{We might want to re-use some of the following (from earlier version):}

A major contribution of our work is to derive closed-form results for
this design criteria for situations where the underlying model is
based on multivariate GPs. Another contribution is that we embed this
approach in a sequential strategy to produce sampling algorithms, and
apply this to a real-world scenario of autonomously sampling
temperature and salinity gradients using an AUV, with the intention of
characterizing the variability of oceanic and riverine waters in
typical mixed coastal environments.

Motivating examples relevant for ESs of multivariate processes are
abundant. In medicine, doctors do not rely solely on a single symptom
but must see several combined effects before making a diagnosis. In
our context of environmental sampling, the salinity and temperature
excursions of a river plume can similarly help characterize the
underlying bio-geochemical processes
\citep{hopkins2013detection,Pinto2018}, where sampling can be aimed
towards reducing uncertainty in the temperature and salinity ES.
Unlike what has been done in previous plume exploration studies, we
emphasize the links to spatial statistical modeling and targeted
multivariate sampling criteria for sequential sampling.

Other statistical
work in the oceanographic domain include \cite{wikle2013modern}
focusing on hierarchical statistical models; \cite{sahu2008space},
studying spatio-temporal models for sea surface temperature and
salinity data; and \cite{mellucci2018oceanic} looking at the
statistical prediction of features using an underwater glider.
In this paper the focus is not on statistical modeling, but rather on statistical results and computations for efficient sampling designs. 

We focus on spatial characterization of a frontal system generated by
river plumes. Fig. \ref{fig:nidelven} shows the survey area in
Trondheim, Norway, where cold freshwater enters from the river,
creating a strong gradient in both temperature and salinity. Because
of the local topography and the Coriolis force the cold fresh water
tends to flow to the east. Depending on the river discharge, tidal
effects, wind, and temperature differences, this boundary often gets
distorted. Initial knowledge about the location and evolution of these
features are highly uncertain, making deterministic planning
challenging.

Adaptive in-situ AUV sampling of an evolving frontal feature has been
explored in \cite{fronts11,Zhang2012,Pinto2018,costa19}. These
approaches typically use a reactive-adaptive scheme, whereby
exploration does not rely on a statistical model of the environment,
but rather adaptation is based on closing the sensing and actuation
loop. Myopic sampling, i.e. stage-wise selection of the path (on the
waypoint graph), has been used for surveys
\citep{singh2009efficient,Binney2013} that focus largely on reducing
predictive variance or entropy. These criteria are widely adopted in
the statistics literature on spatio-temporal design as well
\cite{bueso1998state,zidek2019monitoring}, but variance and entropy
reduction are independent of the actual data realizations under the
assumptions of GP models, so it has limited
decisional flexibility. The use of data-driven adaptive criteria
was introduced to include more targeted sampling of regions of
scientific interest in \cite{Low2009} and \cite{fossuminformation}. In
this paper, we focus on mapping the river plume by rewarding designs
that improve the classification of ES in temperature and salinity.

\begin{comment}
\subsection{Outline of the paper and its appendices / supplemental material}

\begin{itemize}
    \item 
\end{itemize}
\end{comment}

The remainder of this paper is organized as follows: 
%Section \ref{sec:bg} provides select background on ocean sampling. 
Section \ref{sec:ESEP} defines ESs, EPs, and the design criteria of IBV for
vector-valued GPs. Section \ref{sec:heuristics} builds on these
assumptions when deriving the sequential design criteria for adaptive
sampling. Section \ref{sec:simulations} discusses properties of the
methods in simulation studies. Section \ref{sec:case_study}
demonstrates the methodology used in field work characterizing a river
plume and finally, Section \ref{sec:concl_disc} contains a summary and
a discussion of future work.
%\input{section2}





\section{Quantifying uncertainty on Excursion Sets implicitly defined by Gaussian processes}
\label{sec:ESEP}

In section \ref{sec:bg_and_notation} we introduce the notation used in the rest of the paper, review (co-)kriging of multivariate gaussian random fields (GRFs) and present update formulae for cokriging.
Uncertainty quantification (UQ) techniques on excursion sets of GRFs are presented in \ref{sec:set_uq}, in particular the Integrated Bernoulli Variance (IBV) and the excursion measure variance (EMV) are defined and semi-analytical expressions easing their computation and optimization are derived.
Section \ref{sec:eibv} turns to the  effect of new observations on EMV and IBV. Specifically, the expected effect of the inclusion of a new set of observations on the EMV and IBV are considered and further semi-analytical expressions for the expected 
uncertainty reduction are derived. The resulting formulae form the backbone of the uncertainty reduction strategies presented in Section \ref{sec:heuristics}.
Finally, Section \ref{Sec:UnivarEx} illustrates the application of the above concepts on a bivariate example relevant for sampling in the temperature and salinity application.

\subsection{Background, Notation and Co-Kriging}
\label{sec:bg_and_notation}

Throughout the rest of the article, we denote by $\gp$ a vector-valued random field indexed by some arbitrary domain $\domain$, and assume values of the field at any fixed location $\x \in \domain$, denoted $\gp[\x]$, to be a $\no$-variate random vector ($\no\geq 2$). In our leading river plume characterization application, $\domain$ is a prescribed domain in the fjord of Trondheim (for the purpose of our AUV application, a discretization of a $2$-dimensional domain at fixed depth is considered), while $\no=2$ and the two responses of interest stand for temperature and salinity. A bivariate gaussian random field model is assumed for $\gp$, see the two first panels of Figure~\ref{example_excu} for an illustration of a realization of such a vector-valued model (RK NOTATION). The right panel of this figure represents a by-product of interest derived from these realizations, namely regions i) in red, where both temperature and salinity are high (i.e., exceeding respective thresholds), standing for ocean water ii) in pink, where both temperature and salinity are low, standing for riverine water, and iii) in white, where one variable is above and the other below their respective thresholds, standing for mixed waters. 

\begin{figure}[h!] \centering
  \includegraphics[width=0.99\textwidth]{Figures/example_excu_1.png}
  \caption{Realization of a bivariate GRF and excursion set above some threshold. Joint excursion in red, excursion of a single variable in pink.}
\label{example_excu}
\end{figure}

%The methods we develop are fairly general, and not limited to the special case of two-dimensional random fields. Hence, the rest of this section will consider generic excursion sets of gaussian random fields with an arbitrary number of output dimensions.
% Say we have an underlying phenomenon that is modeled as a $\no$-variate gaussian random field $\gp$ on some domain $\domain$, 
Coming back to the more general settings of a $\no$-variate random field, 
we are interested in recovering the set of locations $\es$ in the domain for which the response variables (the components of $\gp$) lie in some set of specified values $\T\subset \mathbb{R}^{\no}$, in other words \textit{the pre-image of $T$ by $\gp$}:
$$
\es:=\gp^{-1}(\T)=\{\x \in \mathcal{M}: \gp[\x] \in \T\}.
$$
%
%Our goal here is to develop approaches to quantify and improve the
%characterization of uncertainties on $\es$. \textcolor{red}{Alread mention how observations are included.}

If we assume that $\gp$ has
continuous trajectories (almost surely) and $T$ is closed, then
$\es$ becomes a Random Closed Set
\citep{Molchanov2005} and concepts from the theory of random sets will prove useful in characterizing the distribution of the volume of $\es$ under some measure defined on the domain.
%\textcolor{red}{Is this really needed?} 
Note that while some aspects of the developed approaches do not call for a
specific form of $\T$, we will often stick for simplicity to the case
of orthants
($\T=(-\infty, t_1] \times \dots \times (-\infty, t_{\no}]$ where
$t_1,\dots, t_{\no} \in \R$) as this will allow efficient calculation
of several key quantities. Note that changing some $\leq$ inequalities
to $\geq$ ones would lead to immediate adaptations.

\medskip

%\subsubsection{Notation}
%In order to work at this level of generality, some notational tricks are needed. Indeed, when considering a new observation, one shall in general specify which component of the field was observed and where it was observed. This leads us to introduce the concept of \textit{generalized location}.

Letting $\gp[\spatloc,\ell]$ denote the $\ell\text{-th}$ component of $\gp[\spatloc]$, $\spatloc\in \domain$ and $\ell \in \{1\dots,p\}$, we will call \textit{generalized location} the couple $x=(\spatloc,\ell)$. 
%Given such a generalized location notation $x$,  
The notation $\gp[x]$ will be used to denote $\gp[\spatloc,\ell]$ 
and
%
%This slight change of notation 
will allow us to think of $\gp$ as a scalar-valued Gaussian random field 
indexed by $D\times \{1\dots,p\}$, which will give the co-kriging equations a particularly simple form that parallels the one of univariate kriging. Due to the naturality of the concept of \textit{generalized location}, 
we will generally use the word \textit{location}, while \textit{spatial location} will be used to stress that we are talking about a point $\spatloc \in \domain$. In the following, the letter $x$ will be reserved for generalized locations, while the letters $\spatloc$ and $\ell$ will be used for spatial locations and response indices respectively.

It turns out that the inclusion of several observations at a time (a batch) may be handled by the same equations as the 
one for a single observation, provided some notation adjustments are made. This motivates the following.



Given a dataset consisting of  $q$ observations at spatial locations $\spatloc_i \in \domain$ and response indices $l_i \in \lbrace 1, ..., \no\rbrace$, $i=1, ..., q$, we can concatenate the information needed to specify the dataset by using the notation
\begin{align*}
\bm{x}:=(\bm{\spatloc}, \bm{\ell}):= (x_1,\dots, x_q),~\text{with }x_i=(\spatloc_i,\ell_i).
\end{align*}
In general, boldface letters will be used to denote concatenated quantities corresponding to batches of observations. 
We can then also compactly denote the values of the field at those different locations by
\begin{align*}
\gp[\bm{x}]:=
\left(\gp[\spatloc_1,\ell_1], ...,
\gp[\spatloc_q,\ell_{q}]\right) \in \mathbb{R}^{q}.
\end{align*}
%
\textcolor{red}{(ADDENDUM, C.T.) In the same fashion, we will use $\mu(\bm{x})$ to denote the $q$-dimensional vector corresponding to the mean at $\bm{x}$ and $k(\bm{x}, \bm{x})$ for the corresponding $q \times q$ covariance matrix.}



Given a random field $\gp$ and (noisy) observations of some of its components at some points in the domain, one can predict the value of the field at some unobserved location $\spatloc\in \domain$ by using the conditional mean of $\gp[\spatloc]$, conditional on the data. This process is called (co-)kriging, and kriging equations precisely tell us how to compute conditional means and covariances conditional on an arbitrary dataset.
We will here present the most general form of cokriging, where one can include several observations (batch) at a time and 
observations at a given location $s \in \domain$ may only include a subset of the components of $\gp[\spatloc]\in\mathbb{R}^{\no}$ 
(heterotopic).

\subsubsection{Co-Kriging}
\label{sec:cokriging}
\textcolor{red}{(C.T.) Why mention batches? We do not care. We have previously available data at $\bm{x}$ and want to predict at $\bm{x}$'. No need to introduce concepts we will never use. Moreover, this presentation suggest that to go from n to n+1 (batch-) observations, we have to condition on the whole $n$ dataset, instead of only including the last step.}

Assuming now that $n$ batches of observations are available, with respective sizes $q_1,\dots, q_n$, and that one wishes 
to predict $\gp[\bm{x}]$ for some batch of $q\geq 1$ generalized locations $\bm{x}=(\bm{\spatloc}, \bm{\ell})$, the (simple) 
cokriging mean would then amount to simple kriging with respect to a scalar-valued Gaussian random field indexed by 
$D\times \{1\dots,p\}$ and with covariance kernel $k(\bm{x}, \bm{x}')=K(\bm{\spatloc}, \bm{\spatloc}')_{\ell, \ell'}$, that is:
%
\begin{equation}\label{eq:cokrig_mean}
\mu_{[n]}(\bm{x})=\mu(\bm{x})+\lambda_{[n]}(\bm{x})^T (\mathbf{z}_{[n]}-\mu(\bm{x})),
\end{equation}
where $\mu$ is $Z$'s initial mean function, $\mathbf{z}_{[n]}$ stands for the ($\sum_{i=1}^n q_i$)-dimensional vector of 
observed responses of $Z$ at all considered generalized locations, and $\lambda_{[n]}(\bm{x})$ is a vector of weights 
equal to $k(\bm{x}_{[n]}, \bm{x}_{[n]})^{-1} k(\bm{x}_{[n]}, \bm{x})$ with $\bm{x}_{[n]}=(\bm{x}_1,\dots, \bm{x}_n)$, 
$k(\bm{x}_{[n]}, \bm{x}_{[n]})$ being assumed non-singular throughout the presentation. The co-kriging %(conditional) 
residual (cross-)covariance function (with respect to batches of generalized locations) can also be expressed in the same vein via
%
\begin{equation}\label{eq:cokrig_cov}
k_{[n]}(\bm{x},\bm{x}')=k(\bm{x},\bm{x}')-\lambda_{[n]}(\bm{x})^T k(\bm{x}_{[n]}, \bm{x}_{[n]}) \lambda_{{[n]}}(\bm{x}').
\end{equation}

\subsubsection{Co-kriging update formulae}

Let us now consider the case where co-kriging prediction of $Z$ was made with respect to $n$ batches $\bm{x}_i$ of generalized locations, concatenated again within
$\bm{x}_{[n]}=(\bm{x}_1,\dots, \bm{x}_n)$, and one wishes to update the prediction by incorporating a new vector of observations $\mathbf{z}_{n+1}$ measured at a batch of $q_{n+1} \geq 1$ generalized locations $\bm{x}_{n+1}$.
%It turns out that the concept of \textit{generalized location} makes the kriging formulae form-invariant across all dimensions. This allows us to directly adapt the 
Thanks to our representation of co-kriging in terms of simple kriging with respect to generalized locations, a strightforward adaptation of the batch-sequential kriging update formulae from \cite{Chevalier.etal2013a} delivers that
% 
\begin{equation}
\mu_{[n+1]}(\bm{x})=\mu_{[n]}(\bm{x})+\lambda_{[n+1,n+1]}(\bm{x})^T (\mathbf{z}_{n+1}-\mu(\bm{x}_{n+1})),
\end{equation}
where $\lambda_{[n+1,n+1]}(\bm{x})$ denotes the $q_{n+1}$-dimensional sub-vector extracted from
$\lambda_{[n+1]}(\bm{x})$ that corresponds to the kriging weigths associated with the last $q_{n+1}$ responses when 
predicting at $\bm{x}$ relying on all measurements until batch $(n+1)$.
%\text{th}$ batch.
%, i.e. those from the $(n+1)\text{th}$ batch of measurements conducted at $\bm{x}_{n+1}$.
Similarly, the updated co-kriging residual (cross-)covariance function then writes
\begin{equation}
k_{[n+1]}(\bm{x},\bm{x}')=k_{[n]}(\bm{x},\bm{x}')-\lambda_{[n+1,n+1]}(\bm{x})^T k_{[n]}(\bm{x}_{[n]}, \bm{x}_{[n]}) \lambda_{{[n+1,n+1]}}(\bm{x}').
\end{equation}
%\medskip
%
Let us remark that, as noted in \cite{Chevalier2015} in the case of scalar-valued fields, these update formulae naturally 
extend to Universal Kriging in second-order settings and apply without Gaussian assumption. We will now see how the latter formulae are instrumental in deriving semi-analytical formulae for stepwise uncertainty reduction criteria for 
vector-valued random fields.






\subsection{Uncertainty Quantification on Excursion Sets of multivariate Gaussian Random Fields}
\label{sec:set_uq}
We now introduce quantities that allow to quantify the uncertainty on the volume of the excursion set $\es$. Let $\mes$ be a 
(locally finite, Borel) measure  on $\domain$. We want to investigate the probability distribution 
of $\mes(\es)$ through its moments.

\medskip

Centred moments may be computed using Proposition~\ref{propo1} developed in the appendix. 
In particular, it allows to write the excursion measure variance $\emv = \operatorname{Var}[\mes(\es)]$ as an integral of the excursion probability
\begin{equation*}
\begin{split}
\emv
&=\int_{\domain^2} \mathbb{P}\left(
\gp[u]\in T, \gp[v]\in T \right)
d\mes^{\otimes}(u, v)\\
&-\left( \int_{\domain} \mathbb{P}\left(\gp[u]\in T\right) d\mes(u) \right)^2,
\end{split}
\end{equation*}
which boils down in the excursion/sojourn case where $\T=(-\infty, t_1] \times
\dots \times (-\infty, t_{\no}]$ to
\begin{equation*}
\begin{split}
\emv
%\operatorname{Var}[\mes(\es)]
&=\int_{\domain^2}
\varPhi_{2\no}
\left(
(\bt, \bt); \mu((u,v)),
K((u,v),(u,v))
\right)
\
\mathrm{d}\mes^{\otimes} %\mes
%\productMeasure
(u,v)\\
&-\left( \int_{\domain} \varPhi_{\no}\left(\bt;\mu(u), K(u)\right) d\mes(u) \right)^2,
\end{split}
\end{equation*}
%
Note that like in the case of scalar-valued fields, this quantity requires to work out an integral over $\domain^2$. In 
contrast, and still like in the scalar-valued case, the Integrated Bernoulli Variance (IBV) of \cite{bect2019} involves 
solely an integral on $\domain$ and can be expanded in our present settings as follows
\begin{equation*}
\begin{split}
\operatorname{IBV} %(\es) %;\mes)
&=\int_{\domain}
\mathbb{P}\left(\gp[\uu]\in T\right)(1-\mathbb{P}\left(\gp[\uu]\in T\right))
d\mes(u) \\
&=\int_{\domain}
\varPhi_{\no}\left(\bt;\mu(\uu), K(\uu)\right)
-\left(\varPhi_{\no}\left(\bt;\mu(\uu), K(\uu)\right) \right)^2
\mathrm{d}\mes(u).
\end{split}
\end{equation*}
%
Next we investigate uncertainty reduction criteria related to those indicators.






\subsection{Expected Integrated Bernoulli Variance and Excursion Measure}
\label{sec:eibv}
This section is devoted to computing the expected effect of the inclusion of new observations on the $\emv$ and $\ibv$ of the excursion set $\es$. Let us consider the same setting as in Section \ref{sec:cokriging} where $n$ batches of measurements have already been performed, and let 
$\currentExp{.}$ and $\currentProba{.}$ denote conditional expectation
and probability conditional on the first $n$ batches of observations, respectively. We want to compute the effect of the inclusion of a new set of observations at $\bm{x}_{n+1}$ on the $\emv$ and $\ibv$.


For uncertainty functional such as $\ibv$ and $\emv$, we use a similar indexing system
with respect to a sequence of batch and use $\IBV_{\stage}$ to denote $\IBV$ with respect to the conditional law $\mathbb{P}_{\stage}$.

\medskip

In order to study the effect of the inclusion of a new data point, we let
$
\currentIBV(\bm{x}; \bm{y})
$
denote the IBV under the current law of the field, conditioned on observing $\bm{y}$ at $\bm{x}$ (generalized, possibly batch observation). The expected effect of a new observation on the IBV is then captured by
\begin{equation}\label{def:eibv}
    \currentEIBV(\bm{x}):=\mathbb{E}_{\stage}\left[\textrb{IBV}(\bm{x}; \bm{Y})\right]
\end{equation}
where $\bm{Y}$ is distributed according to the current law of $Z_{\bm{x}}$.


\textcolor{red}{
The philosophy is the following: EIBV and EEMV are functionals of a probability distribution. Then, if not mentioned explicitly, the probability distribution will be taken to be the current law of the field. (This notation is highly efficient).}


We next present a result that allows to compute $\EIBV$ as an integral of CDFs of multivariate gaussians. This will prove useful when designing sequential uncertainty reduction strategies.

\begin{propo}
\label{propo_eibv}
\begin{equation}
\begin{split}
\currentEIBV(\bm{x})
&=\int_{D} \varPhi_{\no}\left(\bt;~\currentMean{\uu}, \currentCov{u, u}\right) d\mes(u)\\
&-\int_{D} \varPhi_{2\no}
\left(
\left(
\begin{matrix}
\bt-\currentMean{u}\\
\bt-\currentMean{u}
\end{matrix}
\right);
\mathbf{\Sigma}_{[n]}(\uu)
\right)
d\mes(u),
\end{split}
\end{equation}
where the matrix $\mathbf{\Sigma}_{[n]}(\uu)$ is defined as
\begin{equation*}
\begin{split}
\mathbf{\Sigma}_{[n]}(\uu)&=
\left(
\begin{matrix}
\currentCov{u, u} & \currentCov{u, u}-\futureCov{u, u}\\
\currentCov{u, u}-\futureCov{u, u} & \currentCov{u, u}
\end{matrix}
%\begin{matrix}
%\currentCov{u, u} & \Delta_{[n]}(\uu)\\
%\Delta_{[n]}(\uu) & \currentCov{u, u}
%\end{matrix}
\right).\\
\end{split}
\end{equation*}
\end{propo}

As for the expected excursion measure variance, a similar result may be derived.
\begin{propo}
\label{propo_emv}

\begin{equation*}
\begin{split}
\eemv_{[n]}(\bm{x})
&=\int_{D^2} 
\varPhi_{2\no}
\left(
(\bt, \bt); \mu((u,v)), 
K((u,v),(u,v))
\right) 
\
\mathrm{d}\mes^{\otimes} %\mes 
%\productMeasure 
(u,v)\\
&-\currentExp{
    \varPhi_{\no}\left(a_1+B_1 V; C_1\right)
    \varPhi_{\no}\left(a_2+B_2 V; C_2\right)
},
\end{split}
\end{equation*}

with $V=\gp[\bm{x}_{n+1}]-\currentMean{\bm{x}_{n+1}} \sim \mathcal{N}(0_{q_{n+1}},k_{[n]}(\bm{x}_{n+1},\bm{x}_{n+1}))$ and $a_1=\bt-\currentMean{\uu}$,
$B_1=-\lambda_{[n+1,n+1]}(\uu)^T$, $a_2=\bt-\currentMean{\vv}$, $B_2=-\lambda_{[n+1,n+1]}(\vv)^T$.

This integrand finally boils down to an evaluation of the $2\no$-dimensional Gaussian CDF by
Proposition~\ref{propo3}. Details are omitted here for brevity as we did not implement further
this criterion that requires numerical integration over $\domain^2$.
\end{propo}


Note that the calculation of the multivariate Gaussian cumulative distribution
may be done effectively using code such as that of
\cite{genz2009computation}. 

\begin{remark}
Propositions \ref{propo_eibv} and \ref{propo_emv} are twofold generalizations of  results from \cite{chevalier2014fast}: they extend previous results to the multivariate setting and also allow for the inclusion of batch or heterotopic observations through the concept of generalized locations. Related closed-form solutions have been noted in similar contexts \citep{bhattacharjya2013value,stroh}, but not generalized to our situation with
random sets for vector-valued GPs.
\end{remark}












\subsection{Expected Bernoulli Variance on a two dimensional Example}
\label{Sec:UnivarEx}
We illustrate the EBV associated to different designs on a simple bivariate example. The example mimics our river plume application and hence the first and second component of the random field will be called \textit{temperature} and \textit{salinity} for illustrative purpose. Effects of the hyperparameters of the GRF prior on the excursion probabilities will also be studied. For simplicity, we begin with a \textit{pointwise} example, considering a single bivariate gaussian distribution (no spatiality).

\subsubsection{A pointwise study}
Say we want to study the excursion probability of a bivariate gaussian, where the excursion set is defined as
\[
\Gamma := \lbrace \spatloc \in \domain: Z_{\spatloc, 1} \geq t_1, Z_{\spatloc, 2}\geq t_2\rbrace
\]
and the thresholds are set equal to the mean; $\mu_1=t_1=5^o C$ for temperature and  $\mu_2=t_2=30$ mg/l for salinity, and we play with the temperature and salinity correlation and variances to study the effect on the EP and expected Bernoulli variance.

% To BE COMMENTED (TEMPORARILY?)
Fig. \ref{illus_bivarDens} shows contour plots of three different
densities with increasing correlation $\gamma$ between temperature and
salinity. 
\begin{figure}[h!] \centering
  \includegraphics[width=0.99\textwidth]{Figures/illus_bivar.pdf}
  \caption{Density contour plots with different correlations between
    temperature and salinity. The densities have unit variance and the
    thresholds are identical to the mean values $5^o C$ and
    $30 mg/l$. X-axis is temperature and y-axis is salinity.}
\label{illus_bivarDens}
\end{figure}
The displayed densities have unit standard deviations for both
temperature and salinity, but we also study the effect of doubling the
standard deviations.
% END TO BE COMMENTED (TEMPORARILY?)

Table \ref{tab:sim_rhoab} shows the initial EPs and the associated
Bernoulli variance (second row) for the examples indicated in Fig.
\ref{illus_bivarDens}. The EPs increase with the correlation as there
is a strong tendency to have concurrently low temperature and salinity. The Bernoulli variance is similarly large for high
correlations. EPs and Bernoulli variances are the same for standard
deviation $1$ or $2$, which implies that high variability in
temperature and salinity is not captured in the $p(1-p)$ expression.

\begin{table}[!h] \centering \caption{EP and Bernoulli variance for
    different correlations and variances (top rows), and expected
    Bernoulli variances for both temperature and salinity data $\by$ and 
    temperature $y_2$ (bottom rows).}
  \begin{tabular}{c|ccc|ccc}
 &\multicolumn{3}{c}{$\sigma_1=\sigma_2=1$} & \multicolumn{3}{c}{$\sigma_1=\sigma_2=2$} \\
\hline
Correlation $\gamma$ & 0.2 & 0.6 & 0.8 & 0.2 & 0.6 & 0.8 \\
\hline
$p$ & 0.28 & 0.35 & 0.40 & 0.28 & 0.35 & 0.40 \\ 
$p(1-p)$ & 0.20 & 0.23 & 0.24 & 0.20 & 0.23 & 0.24 \\ 
EIBV, Temp and Salinity data & 0.092 & 0.089 & 0.085 & 0.052 & 0.051 & 0.049 \\ 
EIBV, Temperature data only & 0.151 & 0.138 & 0.123 & 0.137 & 0.114 & 0.093 \\ 
\hline
\end{tabular}
\label{tab:sim_rhoab}
\end{table}

Table \ref{tab:sim_rhoab} (bottom two rows) shows results of expected
Bernoulli variance calculations. This is presented for a design
gathering both data types, and for a design with temperature
measurements alone. When both data are gathered, the measurement model is
$(Y_{d,1},Y_{d,2})^t=(Z_1,Z_2)^t+\bepsilon$, with $\bepsilon \sim N(0,0.5^2I_2)$, while $Y_d=Z_1+\epsilon$, $\epsilon \sim N(0,0.5^2)$ when only temperature is measured.
For this illustration, Table \ref{tab:sim_rhoab} shows that the
expected Bernoulli variance gets lower with larger standard deviations
$\sigma_1$ and $\sigma_2$ (right columns). The reduction of Bernoulli
variance is largest for the cases with high correlation
$\gamma$. Albeit smaller, there is also uncertainty reduction when
only temperature is measured (bottom row), especially when temperature
and salinity are highly correlated. When correlation is low
($\gamma=0.2$), there is little information about salinity in the
temperature data, and therefore less uncertainty reduction. In an
application with fresh cold water from a river source, the temperature
and salinity variables will not only be interdependent, but will also
likely show dependence in the spatial dimension. This in turn will
impact the design criteria when we evaluate the information measure by
integrating over several locations (Section \ref{sec:simulations}).

\subsubsection{Including Spatiality}
INTRODUCTORY SENTENCE.
The class of GRF model we will consider in the article will generally have a linear trend
\begin{align*}
\mu(s)=\mathbb{E}\left[\begin{pmatrix}
Z_{s, 1}\\ Z_{s, 2}
\end{pmatrix}\right] &= \beta_0 + \beta_1 s
\end{align*}
with $\beta_0$ a two dimensional vector and $\beta_1$ a $2\times 2$ matrix. We will only consider covariance models of separable type
\begin{align*}
\textrm{Cov}\left(Z_{s, i}, Z_{u, j}\right) &= k(s, u) \gamma(i, j),~ \gamma(i, j) = \begin{cases} \sigma_l^2,~ i=j\\
   \gamma_0 \sigma_i \sigma_j,~i\neq j
        \end{cases}
\end{align*}
where $k(., .)$ is one of the traditional spatial covariance kernel (exponential, Mat\'{e}rn, ...) and $\gamma$ defines the cross-covariance structure.

In the accompanying Python code, these modeling assumptions can be
generalized to anisotropic covariance and changing variance levels
across the spatial domain. One can hence, easily run the code to see
if different models result in other sampling designs. Anisotropy and
non-stationary variance are both relevant for the setting with river
plumes, but in practice this requires more parameters to be
specified. With extensive data and prior knowledge, one could also
possibly fit and estimate parameters of more complex multivariate
spatial covariance functions
\citep{gneiting2010matern,genton2015cross}, but that is outside the
scope of the current paper.

In the rest of this section, we will consider a GRF with mean and covariance structure as above and parameters
\begin{align*}
\beta_0 = \begin{pmatrix}
5.8\\ 24.0
\end{pmatrix}, ~ \beta_1 = \begin{pmatrix}
0.0 & -4.0\\
0.0 & -3.8
\end{pmatrix},~ \sigma_1 = \sigma_2 = 2.25, ~ \gamma_0 = 0.2
\end{align*}
and the spatial covariance is given by a Mat\'{e}rn 3/2 kernel with unit variance and range parameter $\lambda=0.5$.
One realization of this GRF is shown in Fig. \ref{example_excu}.

We now study how the expected Bernoulli variance eq.\ref{eq:eibv} associated to data collection at a point changes if only one of the two components of the field is observed. We first draw a realization of the GRF defined above and use it as ground-truth to mimick the real data-collection process. A first data collection step is performed at the location in green, and the data is used to update the GRF model. The expected Bernoulli variance reduction (at each grid point) that would result from observing only one component, or both at the next grid point upwards is then computed an shown in Fig.\ref{fig:ebv_comp}.

\begin{figure}[h!] \centering
  \includegraphics[width=0.99\textwidth]{Figures/ebv_comp_2.png}
  \caption{Pointwise Bernoulli variance reduction for observation of a single or both components of the random field at one location. Data collection locations in green. True excursion set in red. Places where only one response is above threshold are depicted in pink}
\label{fig:ebv_comp}
\end{figure}

Note that plotting the Bernoulli variance reduction at each point might also be used to compare different data collection plans. For example, Fig. \ref{fig:ebv_comp} shows the expected Bernoulli variance reduction associated to a data collection plan along a straight upwards line (static north) and the one associated to a straight horizontal line (static east). Both expectations are computed according to the a priori distribution of the GRF (i.e. no observations have been included yet).

\begin{figure}[h!] \centering
  \includegraphics[width=0.99\textwidth]{Figures/ebv_north_vs_east_2.png}
  \caption{Pointwise Bernoulli variance reduction for two different observation plans. Data collection locations in green. True excursion set is in red. Places where only one response is above threshold are depicted in pink.}
\label{fig:ebv_north_vs_east}
\end{figure}

\begin{remark}
In all the above examples, the spatial domain $\domain$ has been discretized to a set of $n$ grid
locations $\mathcal{M}_g = \{\x_i, i=1,\ldots,n \}$, where each cell
has area $\Delta$; the same grid is used for the waypoint graph for
possible design locations. The EIBV is approximated by sums over all
grid cells.
\end{remark}




\input{section3}




\section{Case Study - Mapping a River Plume}
\label{sec:case_study}

To demonstrate the applicability of using multivariate EPs and the IBV
to inform oceanographic sampling, we present a case study mapping a
river plume with an AUV. The experiment was performed in Trondheim,
Norway, surveying the Nidelva river (Fig. \ref{fig:nidelven}). The
experiments were conducted in late Spring 2019, when there is still
snow melting in the surrounding mountains so that the river water is
substantially colder than the water in the fjord. The experiment was
focused along the frontal zone that runs more or less parallel to the
eastern shore as noted in Fig. \ref{fig:nidelven}.

\subsection{Model Specification}
\label{sec:exp_modeling}

The statistical model parameters were specified based on a short
preliminary survey where the AUV made an initial transect to determine
the trends in environmental conditions and correlation
structures. Based on the initial data, the trend parameters were
estimated by linear regression, where both temperature and salinity
are assumed to increase linearly, going west from the river
mouth. Next, the residuals from the regression analysis were analyzed
to study the fit of the GP model and to specify the covariance
parameters.

%\begin{figure}[!h] 
% \centering 
%\includegraphics[width=0.98\textwidth]{Figures/field-trials/res_diag.pdf}
%\caption{Data analysis from a preliminary trial experiment using the
%  AUV. Left: Residual plot of temperature and salinity along with
%  Gaussian contours. Middle: Empirical CDF (solid) of the quadratic form of
%  the residuals along with the theoretical CDF (dashed) of the $\chi^2$
%  distribution with two degrees of freedom. Right: Empirical variogram
%  of the salinity and temperature data.} \label{fig:parest}
%\end{figure}

\begin{figure}[!h]
  \centering
  \subfigure[Residual plot.]{\includegraphics[width = 0.32\textwidth]{Figures/field-trials/res_diag_a.pdf}\label{fig:parest_a}}
  \hfill
  \subfigure[Empirical CDF.]{\includegraphics[width = 0.32\textwidth]{Figures/field-trials/res_diag_b.pdf}\label{fig:parest_b}}
  \hfill
  \subfigure[Empirical variogram.]{\includegraphics[width = 0.32\textwidth]{Figures/field-trials/res_diag_c.pdf}\label{fig:parest_c}}
  \caption{Data analysis from a preliminary trial experiment using the
    AUV. \ref{fig:parest_a} Residual plot of temperature and salinity
    along with Gaussian contours. \ref{fig:parest_b} Empirical CDF
    (solid) of the quadratic form of the residuals along with the
    theoretical CDF (dashed) of the $\chi^2$ distribution with two
    degrees of freedom. \ref{fig:parest_c} Empirical variogram of the
    salinity and temperature data.}
\label{fig:parest}
\end{figure}

Fig. \ref{fig:parest} summarizes diagnostic plots of this
analysis. Fig. \ref{fig:parest_a} shows a cross-plot of temperature
and salinity residuals after the westerly trends in both salinity and
temperature are subtracted from the data. This scatter-plot of joint
residuals indicates larger variability in salinity than in
temperature, and a positive correlation ($0.5$) between the two
variables. Based on the fitted bivariate Gaussian model (ellipses in
Fig. \ref{fig:parest_a}), we can compute the modeled quadratic form of
the residuals, and if the model is adequate they should be
approximately $\chi^2_2$ distributed. Fig. \ref{fig:parest_b} shows
the empirical cumulative distribution function (CDF) of these
quadratic forms (solid) together with the theoretical CDF of the
$\chi^2_2$ distribution. The modeled and theoretical curves are very
similar, which indicates that the Gaussian model fits reasonably
well. Even though there appears to be some clustering in both
Fig. \ref{fig:parest_a} and \ref{fig:parest_b}, the bivariate
diagnostic plots look reasonable and justify a Gaussian
model. Fig. \ref{fig:parest_c} shows the empirical variogram of the
scaled residuals for temperature and salinity. The decay is similar
for the two, and seems to be negligible after about $150$ m.


Based on the analysis in Fig. \ref{fig:parest}, the resulting
parameters are given in Table \ref{tab:experiment_param}. The
regression parameters shown here are scaled to represent the east and
west boundaries of the domain as seen in the preliminary transect
data, and the thresholds are intermediate values. These parameter
values were then used in field trials where we explored the
algorithm's ability to characterize the river plume front separating
the river and fjord water masses, providing a spatial map of this
boundary.

%Mapping the spatial extent of a frontal zones is an important problem for studying many bio-physical interactions in the ocean. The frontal zone is determined by the boundary where plumes of sediments, nutrients, and possibly pollutants spreading from the river outlet meet and interact with adjacent coastal water. Due to the lower density the plumes spread on the surface, creating a front with an sharp gradient in both temperature and salinity. 

\begin{table}[!h]
\centering
\begin{tabular}{lrr}
\toprule
Parameter & Value & Source\\
\midrule
\rowcolor{Gray}
Cross correlation temp. and sal. & 0.5 & AUV observations\\
Temp. variance &  0.20 & AUV observations (variogram)\\
\rowcolor{Gray}
Sal. variance &  5.76 & AUV observations (variogram)\\
Corr. range  & 0.15 km & AUV observations (variogram)\\
\rowcolor{Gray}
River temp. $T_{river}$ & $10.0\,^{\circ}\mathrm{C}$ & AUV observations\\
Ocean temp. $T_{ocean}$ & $11.0\,^{\circ}\mathrm{C}$ & AUV observations\\
\rowcolor{Gray}
River sal. $S_{river}$ & $14.0$ g/kg & AUV observations\\
Ocean sal. $S_{ocean}$ & $22.0$ g/kg & AUV observations\\
\rowcolor{Gray}
Threshold temp. & $10.5\,^{\circ}\mathrm{C}$ & $(T_{ocean}-T_{river})/2+T_{river}$\\
Threshold sal. & $18.0$ g/kg & $(S_{ocean}-S_{river})/2+S_{river}$\\
\rowcolor{Gray}
\bottomrule
\end{tabular}
\caption{Model and threshold parameters from an initial AUV
  survey. Observations were taken across the front while crossing from
  fresh, cold river water to saline and warmer ocean waters. \kc{Might
  be good to expand the terms and not have abbreviations in the table
  above; there's plenty of space.}}
\label{tab:experiment_param}
\end{table}


\subsection{Experimental Setup}

The sampling locations were distributed over an equilateral grid, as
shown in the grey-colored lattice in Fig. \ref{fig:map}. The robotic
platform consisted of a Light AUV \citep{sousa2012lauv}
(Fig. \ref{fig:lauv}) equipped with a 16 Hz Seabird Fastcat-49
conductivity, temperature, and depth (CTD) sensor providing
temperature and salinity
measurements. %The accuracy of the CTD instrument is $\pm 0.0003$ S/m (conductivity) and $\pm0.002\,^{\circ}\mathrm{C}$ (temperature).
The sampling agent was built on top of the autonomous agent framework
Teleo-Reactive EXecutive (\textit{T-REX})
\citep{py10,Rajan12,Rajan12b}, running an instance of the
\textit{myopic} strategy from Section \ref{sec:myopic} to 
control the
AUV and decide between sampling locations.

\textcolor{blue}{TO BE MERGED WITH THE PARA ABOVE? The AUVs considered here are powered untethered platforms, that
operate at $1$-$3$ m/s in the upper water column, where they are free
to move in six degrees of freedom (6 DOF). The in-water operation time
capability depends on survey speed, payload sensors and navigation;
typically this amounts to 8-48 hrs. AUVs typically use single-board
computers (SBCs), like the Raspberry Pi or a multicore GPU NVIDIA
Jetson TX1 (quad-core 1.91 GHz 64-bit ARM machine, a 2-MB L2 shared
cache, and 4 GB of 1600 MHz DRAM) for computation onboard.}

\begin{figure}[!h] 
\centering 
\includegraphics[width=0.98\textwidth]{Figures/harald.jpg}
\caption{The commercially available Light Autonomous Underwater
  Vehicle (LAUV) platform for upper water-column exploration used in
  our experiments.}
\label{fig:lauv}
\end{figure} 

The sampling strategy was designed around the concept of visiting
waypoints sequentially. Arriving at a desired waypoint with new
measurements and an updated model, the AUV triggers the myopic
strategy to evaluate the different design criteria (see
Eq. \eqref{critSEQ}). The waypoint-and-path combination that is
expected to reduce the IBV the most is selected, and upon arrival this
procedure is then repeated. At each stage, it takes the AUV about 30
seconds to evaluate the EIBV for all the possible waypoint-and-path
alternatives.

The AUV was set to start in the south-center part of the waypoint
graph, with the previously outlined GP model of the environment
(Section \ref{sec:exp_modeling}). A survey was set to take
approximately 40 minutes, visiting 15 waypoints on the grid, with the
AUV running near the surface to capture the plume. On its path from
one waypoint to the next, the AUV gathered data regularly, and the GP
model assimilated temperature and salinity data with an update
frequency of 30 seconds, giving about three updates per stage.

\begin{figure*}[!h]
\centering
\subfigure[AUV survey area]{\includegraphics[height=0.41\textwidth]{Figures/field-trials/alt_map.eps}\label{fig:map}}
\hspace{0.3cm}
\subfigure[Temperature tracks]{\includegraphics[height=0.41\textwidth]{Figures/field-trials/auv.pdf}\label{fig:res_both}}

\subfigure[Survey 1]{\includegraphics[height=0.40\textwidth]{Figures/field-trials/auv1_es_sal_ep.pdf}\label{fig:res1}}
\hspace{0.2cm}
\subfigure[Survey 2]{\includegraphics[height=0.40\textwidth]{Figures/field-trials/auv4_es_sal_ep.pdf}\label{fig:res2}}

%\subfigure[ES for Survey 1]{\includegraphics[height=0.41\textwidth]{Figures/field-trials/ep_1.pdf}\label{fig:res3}}\hspace{0.4cm}
%\subfigure[ES for Survey 2]{\includegraphics[height=0.41\textwidth]{Figures/field-trials/ep_4.pdf}\label{fig:res4}}
\caption{Results from mapping the Nidelva river, Trondheim, Norway
  over two survey missions. \ref{fig:map} shows an overview of the
  survey area overlaid with the AUV path in black and dashed
  line. Note the shaded region indicating a typical frontal
  region. \ref{fig:res_both} shows the collected temperature data as
  colored trails. Note waypoint 5 (WP5) which indicates where the two
  surveys diverge. \ref{fig:res1} and \ref{fig:res2} shows the
  collected salinity data overlaid on the final EP, which indicate the
  AUVs statistical impression of the front. For both missions the
  temperature and salinity data correspond with an indication of the
  EP front. \kc{Might want to indicate when the surveys (dates) were
    conducted so people can understand the variability in the EP with
    respect to time.}}
\label{fig:results}
\end{figure*}

\subsection{Results}

Two surveys missions (1 and 2), were run successively from 11:00 AM to
01:00 PM, with a short break in between. The resulting path of the
selected waypoints are shown in the map in Fig. \ref{fig:map}, both
within the expected frontal region (shaded pink). The recorded
temperatures are shown as colored trails in Fig. \ref{fig:res_both},
clearly indicating the temperature difference between fjord and
riverine waters. The salinity data are then shown separately, overlaid
with the estimated EP for each survey in Fig. \ref{fig:res1} and
\ref{fig:res2}.

Both surveys successfully estimated and navigated the separation zone,
crossing the frontal boundary multiple times. As conditions changed
slightly between the two surveys, the resulting path (after waypoint
5) is shown to deviate. Survey 1 continued northwards, tracking the
north-eastern portion of the front, while Survey 2 turned west,
mapping the south-western region.

The final predictions of the front location, represented by
conditional EPs in Fig. \ref{fig:res1} and \ref{fig:res2} as dashed
lines, correspond with one another. In both surveys they yield a
picture of the front being to the west in the southern portions of the
region and gradually bending off toward the north east. The amount of
exploration done by Survey 1 is greater than Survey 2. In Survey 1,
the AUV obtained more detail by going north from waypoint 5, while
Survey 2, coming close to the survey area borders in the south-western
corner, obtained a poorer understanding of the northern parts. A
look-ahead strategy might identify and discourage such choices.

%\begin{figure}[!h] 
% \centering 
%\includegraphics[width=0.48\textwidth]{Figures/envir_ocean.pdf}
%\caption{Ocean observation is moving away from %single-ship sampling
%towards more collaborative networked operations in %order to resolve
%the numerous processes and their interaction.} %\label{fig:envir}
%\end{figure}
%\newpage
\section{Closing remarks}\label{sec:concl_disc}

This work builds on a multidisciplinary effort combining statistical
methods with robotic surveying techniques for oceanographic
applications. We show how observation practices can gain efficiency
and accuracy from the development of statistical techniques for spatial monitoring. We further demonstrate the
opportunities available for real-time multivariable spatial
data gathering and analysis onboard autonomous platforms, which
statisticians can exploit to create general-purpose toolkits for
similar applications.

In particular, we derive and show results for characterizing phenomena
connected to the properties of water masses. The characterization of
uncertainties in random sets is extended with new results for the expected integrated Bernoulli variance
reduction achieved by spatial sampling designs. This is
provided in closed-form for the situation with a static design, and
then extended to the adaptive situation. The sequential derivations
provide new insights into efficient applications of adaptive data collection,
as demonstrated in our application.

The case study consider the upper water column in the river plume, represented by a two dimensional grid. Extensions to three-dimensional domains are not methodologically different, but the operation must likely approximate calculations by integrating terms only in the vicinity of the autonomous vehicle \citep{fossum18b}. We did not consider any temporal effects, which would be
relevant on a larger time scale. We consider the extension to
spatio-temporal modeling as future work, and envision that
advection-diffusion equations could be useful in this kind of modeling
\citep{sigrist2015stochastic,richardson2017sparsity}. For more complex oceanographic
phenomena, the methods will need to be extended to non-Gaussian
phenomena, possibly feature-based mixtures of Gaussian processes which could still be
run onboard and augmented by dynamical models. 

The spatial-statistical design criterion building on random sets is
relevant in our setting with different water properties. 
We show
mathematical generality beyond the expected integrated Bernoulli variance, for instance that of volume uncertainties which is possibly more relevant, but requires more computational resources.
Such criteria could be particularly useful in other oceanographic settings of algal-blooms, anoxic
zones or open water fronts \cite{costa19}.
Of course, other criteria
are also relevant for instance, hybrid or multi-attribute criteria
that could balance goals of exploration and exploitation in this
situation. Equally, such techniques have significant use cases in
downstream decision-making, with policy makers and regulators who need
to make difficult decisions related to fish farming or other marine
resource operations, and value of information analysis
\citep{Eidsvik:15} could be used to evaluate whether
information is likely to result in improved decision-making.

\kc{How about talking
  about removing any grids or waypoints altogether?}

In our context the myopic strategy perform rather well, and for computational reasons we did not go in depth on the dynamic program solution. There has been much creative work on finite horizon optimization in the robotics literature including probabilistic road maps and rapidly-exploring random trees \citep{karaman2011sampling}, but their statistical properties are not yet clear.
It is equally interesting to
explore the additional flexibility that can be gained by having
multiple vehicles co-temporally exploring a spatial or spatio-temporal
domain \citep{ferreira2019advancing}. Such an approach would enable
concurrent sampling in different parts of the space, or opportunities
to move in parallel to best capture the excursion set.
The sometimes conflicting topics of autonomy and communication have caught much interest in control engineering lately \citep{zolich2019survey}, and statistical evaluations could contribute in the future. In our context, autonomy is highlighted, and this is reflected in our modeling and computational assumptions. With the possibilities of wifi communication to a mother-ship at waypoints, more advanced modeling and computational routines are of course possible, but with new challenges related to design and planning for when and where the communication should occur. 


\section*{Acknowledgements}

TOF acknowledges support from the Centre for Autonomous Marine
Operations and Systems
(AMOS)\footnote{\url{https://www.ntnu.edu/amos}}, Center of
Excellence, project number 223254, and the Applied Underwater Robotics
Labortatory (AURLab). JE and KR acknowledge support from Norwegian
research council (RCN), project number 305445. CT and DG acknowledge
support from the Swiss National Science Foundation, project number
178858.

%\begin{supplement}
%\sname{Supplement A}\label{suppA}
%\stitle{Title of the Supplement A}
%\slink[url]{http://www.e-publications.org/ims/support/dowload/imsart-ims.zip}
%\sdescription{Dum esset rex in
%accubitu suo, nardus mea dedit odorem suavitatis. Quoniam confortavit
%seras portarum tuarum, benedixit filiis tuis in te. Qui posuit fines tuos}
%\end{supplement}

% == Adding references
\footnotesize
\bibliographystyle{imsart-nameyear}
\bibliography{ref}


\section*{Appendix}
\input{appendix}



% AOS,AOAS: If there are supplements please fill:
%\begin{supplement}[id=suppA]
%  \sname{Supplement A}
%  \stitle{Title}
%  \slink[doi]{10.1214/00-AOASXXXXSUPP}
%  \sdatatype{.pdf}" 
%  \sdescription{Some text}
%\end{supplement}

% === Not used
% After reviews: - This is moved to Section 2.1. 

%Traditional data collection at sea has typically been based on static
%buoys, Lagrangian floats, or ship-based methods, with significant
%logistical limitations that directly impact coverage and sampling
%resolution. Modern methods using satellite remote-sensing provide
%large-scale coverage but have limited resolution, are limited to
%sensing the surface of the ocean, and are impacted by cloud cover. The
%advent of robust mobile robotic platforms \citep{Bellingham07} has
%resulted in significant contributions to environmental monitoring and
%sampling. In particular, autonomous underwater vehicles (AUVs), have
%advanced the state of sampling and consequently have made robotics an
%integral part of ocean observation; our previous work has contributed
%to this effort \citep{das11b,Das2015,fossum18b,fossuminformation}. Other ¤ %statistical work in the oceanographic domain include \cite{wikle2013modern}
%focusing on hierarchical statistical models; \cite{sahu2008space},
%studying spatio-temporal models for sea surface temperature and
%salinity data; and \cite{mellucci2018oceanic} looking at the
%statistical prediction of features using an underwater glider.

%\begin{itemize}
%
%\item Full numerical ocean models cannot provide accurate results
%  online if run on robotic sensing platforms, as onboard computers
%  cannot deliver the computational power required. Hence statistical
%  proxy models of the environment must be used for learning where to sample.
%
%\item With limited available information about the state of the ocean, there is substantial value in reacting to
%  information obtained from measurements taken in-situ. This
%  acquired information must be assimilated into statistical models that
%  can be used to inform decisions on where to sample
%  sequentially.%\emph{in-situ}; this is usually referred to as the \emph{adaptivity gap} \citep{ause2008phd}.
%
%\item For sampling problems related to environmental sensing, the
%  number of choices (i.e. locations, trajectories, and candidate
%  designs) is enormous, creating a
%  trade-off between optimization (finding the most resource-efficient
%  design to collect necessary data) and computability (arriving at a
%  solution in reasonable time). To successfully resolve features, this
%  trade-off has to be considered in development and practice.
%
%\end{itemize}

%Addressing this, the combination of statistical tools and robotic platforms is a
%natural symbiosis which enables information-based sensing. Central to
%this is the ability to model spatially-correlated variables and
%provide formal measures of uncertainty. Our formulation is based on
%Gaussian Processes (GPs) as they allow efficient implementation 
%and evaluation in real time onboard a robotic platform.

%Sampling can, in this context, not simply be distributed evenly ---along simple transects or ``lawn-mover" patterns--- but must instead be prioritized to relevant regions to ensure it is cost-effective while providing adequate coverage and resolution of the area of scientific interest.

%While the focus has often been on
%biological and anthropogenic impact from micro-plastics to
%pollution, biological oceanographers have focused intently on studying
%micro-organisms at the base of the human food web. These organisms are
%critically impacted by the changing dynamics in the upper water-column,
%especially in coastal zones which are complex and often hard to observe
%in space and time. By studying the bio-geochemical processes in the
%upper water-column scientists can measure the impact of change, natural
%or anthropomorphic, and provide an informed opinion to policy makers to
%effect changes in preserving the environment. However, the challenge of 
% The pressure on marine resources is growing and increased accuracy,
% resolution, and persistent monitoring of the oceans is crucial for
% long-term sustainable management. 

% A
% sustained focus on prioritized and efficient data collection strategies
% have therefore started to emerge. The advent of marine robotic
% platforms, especially

% provided means to execute this prioritization through the capacity of
% autonomy and data-driven sampling, where data collection in principle
% can be optimized. These capabilities have made

% of the emerging sensing practice for ocean science, allowing scientists
% to increase the observational efficiency and resolution beyond what was
% previously possible. But how should a robotic platform, such as an AUV,
% effectively prioritize and identify important regions for sampling? The
% answer to this question relates to 

% , and
% the application domain is clearly an arena where statisticians can
% contribute.
% There has recently been some statistical attention in oceanography:

%From an oceanographic perspective, interesting regions are usually directly tied to a distinct phenomena that is of scientific interest. Each phenomenon can in turn be characterized by a set of process specific conditions expressed through different measures of key environmental variables, such as temperature or salinity. One such measure is the gradient, that can be associated with a number of important processes, such as the vertical location of the thermocline and pycnocline, location of upwelling systems, vertical mixing, eddies, fronts, and currents \cite{sverdrup2006}, as well as distribution, growth, and accumulation of biological activity \cite{SatOceanSoci00, Ryan2014}. These gradients create boundaries separating the ocean into process specific regions which are of profound interest to both identify and map effectively. Quantification of gradient features is therefore a much needed competence in robotic sampling.
\end{document}

